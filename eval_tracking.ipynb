{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from models import KeyPointVAE\n",
    "from utils.util_func import reparameterize\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.widgets import Slider, Button\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "#from accelerate import Accelerator, DistributedDataParallelKwargs\n",
    "from utils.util_func import create_tracked_kp_video, plot_matched_keypoints , plot_keypoints_on_image, plot_bb_on_image_from_masks\n",
    "from utils.util_func import create_masks_fast\n",
    "from datasets.calvin_dataset import CalvinDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "image_idx = 6\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "use_logsoftmax = False\n",
    "pad_mode = 'replicate'\n",
    "sigma = 0.1\n",
    "dropout = 0.0\n",
    "n_kp = 1\n",
    "kp_range = (-1, 1)\n",
    "kp_activation = \"tanh\"\n",
    "mask_threshold = 0.2\n",
    "learn_order = False\n",
    "path_to_model_ckpt = './checkpoints/calvin_dlp_gauss_pointnetpp_best.pth'\n",
    "image_size = 128\n",
    "ch = 3\n",
    "enc_channels = [32, 64, 128, 256]\n",
    "prior_channels = (16, 32, 64)\n",
    "imwidth = 160\n",
    "crop = 16\n",
    "n_kp_enc = 10 \n",
    "n_kp_prior = 20 \n",
    "use_object_enc = True\n",
    "use_object_dec = True\n",
    "learned_feature_dim = 10\n",
    "patch_size = 16\n",
    "anchor_s = 0.25\n",
    "dec_bone = \"gauss_pointnetpp\"\n",
    "exclusive_patches = False\n",
    "\n",
    "model = KeyPointVAE(cdim=ch, enc_channels=enc_channels, prior_channels=prior_channels,\n",
    "                    image_size=image_size, n_kp=n_kp, learned_feature_dim=learned_feature_dim,\n",
    "                    use_logsoftmax=use_logsoftmax, pad_mode=pad_mode, sigma=sigma,\n",
    "                    dropout=dropout, dec_bone=dec_bone, patch_size=patch_size, n_kp_enc=n_kp_enc,\n",
    "                    n_kp_prior=n_kp_prior, kp_range=kp_range, kp_activation=kp_activation,\n",
    "                    mask_threshold=mask_threshold, use_object_enc=use_object_enc,\n",
    "                    exclusive_patches=exclusive_patches, use_object_dec=use_object_dec, anchor_s=anchor_s,\n",
    "                    learn_order=learn_order).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(path_to_model_ckpt, map_location=device), strict=False)\n",
    "model.eval()\n",
    "print(\"loaded model from checkpoint\")\n",
    "\n",
    "path_to_images = ['./checkpoints/sample_images/calvin/calvin_scene_A.png',\n",
    "                          './checkpoints/sample_images/calvin/calvin_scene_B.png',\n",
    "                          './checkpoints/sample_images/calvin/calvin_scene_C.png',\n",
    "                          './checkpoints/sample_images/calvin/calvin_scene_D.png',\n",
    "                          './checkpoints/sample_images/calvin/calvin_scene_D_2.png',\n",
    "                          './checkpoints/sample_images/calvin/calvin_scene_D_3.png',\n",
    "                          './checkpoints/sample_images/calvin/calvin_scene_D_4.png']\n",
    "\n",
    "image_idx = min(image_idx, len(path_to_images) - 1)\n",
    "path_to_image = path_to_images[image_idx]\n",
    "im = Image.open(path_to_image)\n",
    "im = im.convert('RGB')\n",
    "im = im.resize((image_size, image_size), Image.BICUBIC)\n",
    "trans = transforms.ToTensor()\n",
    "data = trans(im)\n",
    "data = data.unsqueeze(0).to(device)\n",
    "x = data\n",
    "logvar_threshold = 13.0\n",
    "\n",
    "with torch.no_grad():\n",
    "        deterministic = True\n",
    "\n",
    "        model_output = model(x, x_prior=x)\n",
    "        mu_p = model_output['kp_p']\n",
    "        gmap = model_output['gmap']\n",
    "        mu = model_output['mu']\n",
    "        logvar = model_output['logvar']\n",
    "        rec_x = model_output['rec']\n",
    "        mu_features = model_output['mu_features']\n",
    "        logvar_features = model_output['logvar_features']\n",
    "        # object stuff\n",
    "        dec_objects_original = model_output['dec_objects_original']\n",
    "        cropped_objects_original = model_output['cropped_objects_original']\n",
    "        print(model_output.keys())\n",
    "        print(mu)\n",
    "        print(rec_x)\n",
    "        print(mu_p)\n",
    "        print(mu_features)\n",
    "        print(logvar_features)\n",
    "        '''\n",
    "        enc_out = model.encode_all(data, return_heatmap=True, deterministic=deterministic)\n",
    "        mu, logvar, kp_heatmap, mu_features, logvar_features, obj_on, order_weights = enc_out\n",
    "        '''\n",
    "        if deterministic:\n",
    "            z = mu\n",
    "            z_features = mu_features\n",
    "        else:\n",
    "            z = reparameterize(mu, logvar)\n",
    "            z_features = reparameterize(mu_features, logvar_features)\n",
    "        # top-k\n",
    "        logvar_sum = logvar.sum(-1)\n",
    "        logvar_topk = torch.topk(logvar_sum, k=5, dim=-1, largest=False)\n",
    "        indices = logvar_topk[1]  # [batch_size, topk]\n",
    "        batch_indices = torch.arange(mu.shape[0]).view(-1, 1).to(mu.device)\n",
    "        topk_kp = mu[batch_indices, indices]\n",
    "        # bounding boxes\n",
    "        masks = create_masks_fast(mu[:, :-1].detach(), anchor_s=model.anchor_s, feature_dim=x.shape[-1])\n",
    "        masks = torch.where(masks < mask_threshold, 0.0, 1.0)\n",
    "        bb_scores = -1 * logvar_sum\n",
    "        hard_threshold = bb_scores.mean()\n",
    "\n",
    "        # print(masks[0].shape)\n",
    "        # print(masks[0])\n",
    "        # print(x[0].shape)\n",
    "        # print(bb_scores[0].shape)\n",
    "        # print(bb_scores[0])\n",
    "        # img_with_masks_nms = plot_bb_on_image_from_masks_nms(masks[0], x[0], scores=bb_scores[0])\n",
    "        img_with_masks = plot_bb_on_image_from_masks(masks[0], x[0])\n",
    "        #img_with_masks_nms, nms_ind = plot_bb_on_image_batch_from_masks_nms(masks, x,scores=bb_scores)\n",
    "N = mu.shape[1]\n",
    "xmin = 0\n",
    "xmax = image_size\n",
    "\n",
    "x = np.linspace(xmin, xmax, N)\n",
    "\n",
    "mu = mu.clamp(kp_range[0], kp_range[1])\n",
    "original_mu = mu.clone()\n",
    "mu = (mu - kp_range[0]) / (kp_range[1] - kp_range[0])\n",
    "xvals = mu[0, :-1, 1].data.cpu().numpy() * (image_size - 1)\n",
    "yvals = mu[0, :-1, 0].data.cpu().numpy() * (image_size - 1)\n",
    "if learned_feature_dim > 0:\n",
    "    feature_1_vals = mu_features[0, :, -1].data.cpu().numpy()\n",
    "    # set up a plot for topk\n",
    "\n",
    "topk_kp = topk_kp.clamp(kp_range[0], kp_range[1])\n",
    "topk_kp = (topk_kp - kp_range[0]) / (kp_range[1] - kp_range[0])\n",
    "xvals_topk = topk_kp[0, :, 1].data.cpu().numpy() * (image_size - 1)\n",
    "yvals_topk = topk_kp[0, :, 0].data.cpu().numpy() * (image_size - 1)\n",
    "\n",
    "\n",
    "#print(topk_kp)\n",
    "#print(data.shape)\n",
    "#img_with_kp_topk = plot_keypoints_on_image_batch(topk_kp, data)\n",
    "img_with_kp_topk = plot_keypoints_on_image(topk_kp[0], data[0])\n",
    "plt.imshow(img_with_kp_topk)\n",
    "plt.axis('off') \n",
    "plt.show()\n",
    "\n",
    "img_with_kp = plot_keypoints_on_image(mu[:, :-1][0].clamp(min=kp_range[0], max=kp_range[1]), data[0])\n",
    "plt.imshow(img_with_kp)\n",
    "plt.axis('off') \n",
    "plt.show()\n",
    "\n",
    "'''\n",
    "img_with_kp_p = plot_keypoints_on_image(mu_p[0], data[0])\n",
    "plt.imshow(img_with_kp_p)\n",
    "plt.axis('off') \n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "plt.imshow(img_with_masks)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "dec_objects = model_output['dec_objects']\n",
    "plt.imshow(dec_objects[0].permute(1, 2, 0).cpu().numpy())\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "# dec_objects[:max_imgs, -3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "show image with kps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def showImgWithKP(img, model):\n",
    "    model.eval()\n",
    "\n",
    "    im = img\n",
    "    im = im.convert('RGB')\n",
    "    im = im.resize((image_size, image_size), Image.BICUBIC)\n",
    "    trans = transforms.ToTensor()\n",
    "    data = trans(im)\n",
    "    data = data.unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "            model_output = model(data, x_prior=data)\n",
    "\n",
    "            mu_p = model_output['kp_p']\n",
    "            gmap = model_output['gmap']\n",
    "            mu = model_output['mu']\n",
    "            logvar = model_output['logvar']\n",
    "            rec_x = model_output['rec']\n",
    "            mu_features = model_output['mu_features']\n",
    "            logvar_features = model_output['logvar_features']\n",
    "\n",
    "            #print('kp_p: ', mu_p)\n",
    "            # print('mu: ', mu)\n",
    "            # print('mu_features: ', mu_features)\n",
    "            #print('logvar: ', logvar)\n",
    "            #print('logvar_features: ', logvar_features)\n",
    "            #print('rec: ', rec_x)\n",
    "\n",
    "            # top-k\n",
    "            logvar_sum = logvar.sum(-1)\n",
    "            logvar_topk = torch.topk(logvar_sum, k=10, dim=-1, largest=False)\n",
    "            indices = logvar_topk[1]  # [batch_size, topk]\n",
    "            batch_indices = torch.arange(mu.shape[0]).view(-1, 1).to(mu.device)\n",
    "            topk_kp = mu[batch_indices, indices]\n",
    "\n",
    "    # print('mu: ', mu[0])\n",
    "\n",
    "    topk_kp = mu.clamp(kp_range[0], kp_range[1])\n",
    "    topk_kp = (topk_kp - kp_range[0]) / (kp_range[1] - kp_range[0])\n",
    "\n",
    "    # print('topk_kp: ', topk_kp[0])\n",
    "\n",
    "    img_with_kp_topk = plot_keypoints_on_image(topk_kp[0], data[0])\n",
    "\n",
    "    dec_objects = model_output['dec_objects']\n",
    "    dec_objects = dec_objects[0].permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "    combined_img = np.hstack((img_with_kp_topk / 255.0, dec_objects))\n",
    "\n",
    "    plt.imshow(combined_img)\n",
    "    plt.axis('off') \n",
    "    plt.show()\n",
    "\n",
    "def assosiateKP(img1, img2, model):\n",
    "    model.eval()\n",
    "\n",
    "    img1 = img1.convert('RGB')\n",
    "    img1 = img1.resize((image_size, image_size), Image.BICUBIC)\n",
    "    trans = transforms.ToTensor()\n",
    "    data1 = trans(img1)\n",
    "    data1 = data1.unsqueeze(0).to(device)\n",
    "\n",
    "    img2 = img2.convert('RGB')\n",
    "    img2 = img2.resize((image_size, image_size), Image.BICUBIC)\n",
    "    trans = transforms.ToTensor()\n",
    "    data2 = trans(img2)\n",
    "    data2 = data2.unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model_output_1 = model(data1, x_prior=data1)\n",
    "        model_output_2 = model(data2, x_prior=data2)\n",
    "\n",
    "        mu_coords_1 = model_output_1['mu']\n",
    "        mu_coords_2 = model_output_2['mu']\n",
    "        mu_features_1 = model_output_1['mu_features']\n",
    "        mu_features_2 = model_output_2['mu_features']\n",
    "        logvar_1 = model_output_1['logvar']\n",
    "        logvar_2 = model_output_2['logvar']\n",
    "\n",
    "       # mu_1 = list(zip(mu_coords_1[0], mu_features_1[0]))\n",
    "       # mu_2 = list(zip(mu_coords_2[0], mu_features_2[0]))\n",
    "        mu_1 = list(zip(mu_coords_1[0].cpu().numpy(), mu_features_1[0].cpu().numpy()))\n",
    "        mu_2 = list(zip(mu_coords_2[0].cpu().numpy(), mu_features_2[0].cpu().numpy()))\n",
    "\n",
    "        matched_KPs = []\n",
    "        for kp_1, feature_1 in mu_1:\n",
    "            best_match = None\n",
    "            best_sim = float('inf')\n",
    "            for kp_2, feature_2 in mu_2:\n",
    "                # Euclidean distance \n",
    "                geometric_sim = np.linalg.norm(kp_1 - kp_2)\n",
    "                # Cosine Similarity\n",
    "                cos_sim = np.dot(feature_1, feature_2) / ((np.linalg.norm(feature_1)) * (np.linalg.norm(feature_2)))\n",
    "                semantic_sim = abs(1 - cos_sim)\n",
    "\n",
    "                total_sim = geometric_sim + semantic_sim\n",
    "\n",
    "                if total_sim < best_sim:\n",
    "                    best_sim = total_sim\n",
    "                    best_match = kp_2\n",
    "\n",
    "                # print('simulatiry: ', total_sim, ' : ', geometric_sim, ' : ', semantic_sim)\n",
    "            sim_threshold = 0.2\n",
    "            if best_sim < sim_threshold:\n",
    "                matched_KPs.append((kp_1, best_match, best_sim))\n",
    "\n",
    "        # filter keypoints one-to-one\n",
    "        matched_KPs.sort(key=lambda x: x[2])\n",
    "        matched_KPs_filtered = []\n",
    "        matched_kp_1 = []\n",
    "        matched_kp_2 = []\n",
    "        for kp_1, kp_2, sim in matched_KPs:\n",
    "            if (kp_1[0], kp_1[1]) not in matched_kp_1 and (kp_2[0], kp_2[1]) not in matched_kp_2: \n",
    "                matched_KPs_filtered.append((kp_1, kp_2, sim))\n",
    "                matched_kp_1.append((kp_1[0], kp_1[1]))\n",
    "                matched_kp_2.append((kp_2[0], kp_2[1]))\n",
    "\n",
    "        # print(matched_KPs_filtered)\n",
    "\n",
    "\n",
    "    #topk_kp = topk_kp.clamp(kp_range[0], kp_range[1])\n",
    "    #topk_kp = (topk_kp - kp_range[0]) / (kp_range[1] - kp_range[0])\n",
    "    \n",
    "    matched_kp_tuple = []\n",
    "    for kp_1, kp_2, sim in matched_KPs_filtered:\n",
    "        kp_1 = ((kp_1 - kp_range[0]) / (kp_range[1] - kp_range[0])).clip(0, 1)\n",
    "        kp_2 = ((kp_2 - kp_range[0]) / (kp_range[1] - kp_range[0])).clip(0, 1)\n",
    "        kp_tuple = (kp_1, kp_2)\n",
    "        matched_kp_tuple.append(kp_tuple)\n",
    "\n",
    "    #matched_kp_tuple = torch.Tensor(matched_kp_tuple)\n",
    "\n",
    "    image_matched = plot_matched_keypoints(matched_kp_tuple, data2[0])\n",
    "    plt.imshow(image_matched)\n",
    "    plt.axis('off') \n",
    "    plt.show()  \n",
    "\n",
    "    kp_1_list = []\n",
    "    kp_2_list = []\n",
    "\n",
    "    for kp_1, kp_2, sim in matched_KPs_filtered:\n",
    "        kp_1_list.append(kp_1)\n",
    "        kp_2_list.append(kp_2)\n",
    "\n",
    "        topk_kp_1 = torch.tensor([kp_1]).clamp(kp_range[0], kp_range[1])\n",
    "        topk_kp_1 = (topk_kp_1 - kp_range[0]) / (kp_range[1] - kp_range[0])\n",
    "        img_with_kp_topk_1 = plot_keypoints_on_image(topk_kp_1, data1[0])\n",
    "\n",
    "        topk_kp_2 = torch.tensor([kp_2]).clamp(kp_range[0], kp_range[1])\n",
    "        topk_kp_2 = (topk_kp_2 - kp_range[0]) / (kp_range[1] - kp_range[0])   \n",
    "        img_with_kp_topk_2 = plot_keypoints_on_image(topk_kp_2, data2[0])\n",
    "\n",
    "        combined_img = np.hstack((img_with_kp_topk_1,  img_with_kp_topk_2))\n",
    "\n",
    "        plt.imshow(combined_img)\n",
    "        plt.axis('off') \n",
    "        plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print keypoints for caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder backbone: gauss_pointnetpp\n",
      "prior std: 0.1\n",
      "keypoints range: (-1, 1)\n",
      "total number of kp: 64 -> prior kp: 20\n",
      "number of kp from encoder: 10\n",
      "kp_activation: tanh\n",
      "learnable feature dim: 10\n",
      "mask threshold: 0.2\n",
      "object patch size: 32\n",
      "object encoder: True, object decoder: True\n",
      "learn particles order: False\n",
      "conv shape:  torch.Size([256, 16, 16])\n",
      "conv shape:  torch.Size([32, 8, 8])\n",
      "conv shape:  torch.Size([64, 4, 4])\n",
      "YES\n",
      "loaded model from checkpoint\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'showImgWithKP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/media/tim/D/deep-latent-particles-pytorch/eval_tracking.ipynb Cell 6\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/tim/D/deep-latent-particles-pytorch/eval_tracking.ipynb#W5sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m img_2 \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(np\u001b[39m.\u001b[39muint8(img_arr_2))\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/tim/D/deep-latent-particles-pytorch/eval_tracking.ipynb#W5sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m img_2 \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(np\u001b[39m.\u001b[39muint8(img_arr_2))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/media/tim/D/deep-latent-particles-pytorch/eval_tracking.ipynb#W5sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m showImgWithKP(img_1, model)        \n\u001b[1;32m     <a href='vscode-notebook-cell:/media/tim/D/deep-latent-particles-pytorch/eval_tracking.ipynb#W5sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m assosiateKP(img_1, img_2, model)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'showImgWithKP' is not defined"
     ]
    }
   ],
   "source": [
    "def getIndex(path, ann):\n",
    "        annotations = np.load('{}/lang_annotations/auto_lang_ann.npy'.format(path), allow_pickle=True).item()\n",
    "        annotations = list(zip(annotations[\"info\"][\"indx\"], annotations[\"language\"][\"ann\"]))\n",
    "        for annotation in annotations:\n",
    "                if annotation[1] != ann:\n",
    "                        continue\n",
    "                return annotation[0][0]\n",
    "        return 0\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "use_logsoftmax = False\n",
    "pad_mode = 'replicate'\n",
    "sigma = 0.1\n",
    "dropout = 0.0\n",
    "n_kp = 1\n",
    "kp_range = (-1, 1)\n",
    "kp_activation = \"tanh\"\n",
    "mask_threshold = 0.2\n",
    "learn_order = False\n",
    "path_to_model_ckpt = './checkpoints/calvin_dlp_gauss_pointnetpp_best.pth'\n",
    "image_size = 128\n",
    "ch = 3\n",
    "enc_channels = [32, 64, 128, 256]\n",
    "prior_channels = (16, 32, 64)\n",
    "imwidth = 160\n",
    "crop = 16\n",
    "n_kp_enc = 10 \n",
    "n_kp_prior = 20 \n",
    "use_object_enc = True\n",
    "use_object_dec = True\n",
    "learned_feature_dim = 10\n",
    "patch_size = 16\n",
    "anchor_s = 0.25\n",
    "dec_bone = \"gauss_pointnetpp\"\n",
    "exclusive_patches = False\n",
    "\n",
    "model = KeyPointVAE(cdim=ch, enc_channels=enc_channels, prior_channels=prior_channels,\n",
    "                        image_size=image_size, n_kp=n_kp, learned_feature_dim=learned_feature_dim,\n",
    "                        use_logsoftmax=use_logsoftmax, pad_mode=pad_mode, sigma=sigma,\n",
    "                        dropout=dropout, dec_bone=dec_bone, patch_size=patch_size, n_kp_enc=n_kp_enc,\n",
    "                        n_kp_prior=n_kp_prior, kp_range=kp_range, kp_activation=kp_activation,\n",
    "                        mask_threshold=mask_threshold, use_object_enc=use_object_enc,\n",
    "                        exclusive_patches=exclusive_patches, use_object_dec=use_object_dec, anchor_s=anchor_s,\n",
    "                        learn_order=learn_order).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(path_to_model_ckpt, map_location=device), strict=False)\n",
    "print(\"loaded model from checkpoint\")\n",
    "\n",
    "path = '/media/tim/E/datasets/task_D_D/validation'\n",
    "target_ann = \"pick up the pink block from the drawer\"\n",
    "start_index = getIndex(path, target_ann)\n",
    "        \n",
    "data_1 = np.load(f\"{path}/episode_{start_index + 40:07d}.npz\", allow_pickle=True)\n",
    "img_arr_1 = data_1['rgb_static']\n",
    "img_1 = Image.fromarray(np.uint8(img_arr_1))\n",
    "data_2 = np.load(f\"{path}/episode_{start_index + 41:07d}.npz\", allow_pickle=True)\n",
    "img_arr_2 = data_2['rgb_static']\n",
    "img_2 = Image.fromarray(np.uint8(img_arr_2))\n",
    "        \n",
    "img_2 = Image.fromarray(np.uint8(img_arr_2))\n",
    "showImgWithKP(img_1, model)        \n",
    "assosiateKP(img_1, img_2, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track keypoints in captioned clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "\n",
    "def getIndex(path, ann):\n",
    "        annotations = np.load('{}/lang_annotations/auto_lang_ann.npy'.format(path), allow_pickle=True).item()\n",
    "        annotations = list(zip(annotations[\"info\"][\"indx\"], annotations[\"language\"][\"ann\"]))\n",
    "        for annotation in annotations:\n",
    "                if annotation[1] != ann:\n",
    "                        continue\n",
    "                return annotation[0][0]\n",
    "        return 0\n",
    "\n",
    "def trackKeyPoints(image_array, model):\n",
    "    model.eval()\n",
    "    tracked_keypoints = [] \n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    fps = 20 \n",
    "    frame_size = (128, 128)\n",
    "    out = cv2.VideoWriter('tracked_keypoints_video.avi', fourcc, fps, frame_size)\n",
    "    \n",
    "    print(len(image_array))\n",
    "    for i in range(len(image_array) - 1):\n",
    "        \n",
    "        img1 = Image.fromarray(np.uint8(image_array[i]))\n",
    "        img1 = img1.convert('RGB')\n",
    "        img1 = img1.resize((image_size, image_size), Image.BICUBIC)\n",
    "        trans = transforms.ToTensor()\n",
    "        data1 = trans(img1)\n",
    "        data1 = data1.unsqueeze(0).to(device)\n",
    "\n",
    "        img2 = Image.fromarray(np.uint8(image_array[i + 1]))\n",
    "        img2 = img2.convert('RGB')\n",
    "        img2 = img2.resize((image_size, image_size), Image.BICUBIC)\n",
    "        data2 = trans(img2)\n",
    "        data2 = data2.unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model_output_1 = model(data1, x_prior=data1)\n",
    "            model_output_2 = model(data2, x_prior=data2)\n",
    "\n",
    "            mu_coords_1 = model_output_1['mu']\n",
    "            mu_coords_2 = model_output_2['mu']\n",
    "            mu_features_1 = model_output_1['mu_features']\n",
    "            mu_features_2 = model_output_2['mu_features']\n",
    "\n",
    "            mu_1 = list(zip(mu_coords_1[0].cpu().numpy(), mu_features_1[0].cpu().numpy()))\n",
    "            mu_2 = list(zip(mu_coords_2[0].cpu().numpy(), mu_features_2[0].cpu().numpy()))\n",
    "\n",
    "            matched_KPs = []\n",
    "            for kp_1, feature_1 in mu_1:\n",
    "                best_match = None\n",
    "                best_sim = float('inf')\n",
    "                for kp_2, feature_2 in mu_2:\n",
    "                    # Calculate similarity\n",
    "                    geometric_sim = np.linalg.norm(kp_1 - kp_2)\n",
    "                    cos_sim = np.dot(feature_1, feature_2) / (np.linalg.norm(feature_1) * np.linalg.norm(feature_2))\n",
    "                    semantic_sim = abs(1 - cos_sim)\n",
    "                    total_sim = geometric_sim + semantic_sim\n",
    "                    if total_sim < best_sim:\n",
    "                        best_sim = total_sim\n",
    "                        best_match = kp_2\n",
    "                sim_threshold = 0.2\n",
    "                if best_sim < sim_threshold:\n",
    "                    matched_KPs.append((kp_1, best_match, best_sim))\n",
    "\n",
    "            # Filter keypoints to ensure one-to-one associations\n",
    "            matched_KPs.sort(key=lambda x: x[2])\n",
    "            matched_KPs_filtered = []\n",
    "            matched_kp_1 = []\n",
    "            matched_kp_2 = []\n",
    "            for kp_1, kp_2, sim in matched_KPs:\n",
    "                if (kp_1[0], kp_1[1]) not in matched_kp_1 and (kp_2[0], kp_2[1]) not in matched_kp_2: \n",
    "                    matched_KPs_filtered.append((kp_1, kp_2, sim))\n",
    "                    matched_kp_1.append((kp_1[0], kp_1[1]))\n",
    "                    matched_kp_2.append((kp_2[0], kp_2[1]))\n",
    "            \n",
    "            tracked_keypoints.append(matched_KPs_filtered)\n",
    "\n",
    "        matched_kp_tuple = []\n",
    "\n",
    "\n",
    "        for kp_1, kp_2, sim in matched_KPs_filtered:\n",
    "            kp_1 = ((kp_1 - kp_range[0]) / (kp_range[1] - kp_range[0])).clip(0, 1)\n",
    "            kp_2 = ((kp_2 - kp_range[0]) / (kp_range[1] - kp_range[0])).clip(0, 1)\n",
    "            kp_tuple = (kp_1, kp_2)\n",
    "            matched_kp_tuple.append(kp_tuple)\n",
    "        image_matched = plot_matched_keypoints(matched_kp_tuple, data2[0])\n",
    "        plt.imshow(image_matched)\n",
    "        plt.axis('off') \n",
    "        plt.show()  \n",
    "        out.write(image_matched[:, :, ::-1])\n",
    "\n",
    "        '''\n",
    "        for matched_kp in tracked_keypoints:\n",
    "            matched_kp_tuple = []\n",
    "            for kp_1, kp_2, sim in matched_kp:\n",
    "                kp_1 = ((kp_1 - kp_range[0]) / (kp_range[1] - kp_range[0])).clip(0, 1)\n",
    "                kp_2 = ((kp_2 - kp_range[0]) / (kp_range[1] - kp_range[0])).clip(0, 1)\n",
    "                kp_tuple = (kp_1, kp_2)\n",
    "                matched_kp_tuple.append(kp_tuple)\n",
    "            image_matched = plot_matched_keypoints(matched_kp_tuple, data2[0])\n",
    "            plt.imshow(image_matched)\n",
    "            plt.axis('off') \n",
    "            plt.show()  \n",
    "        '''\n",
    "    out.release()\n",
    "    return tracked_keypoints\n",
    "\n",
    "\n",
    "def trackKeyPoints_2(image_array, model):\n",
    "    model.eval()\n",
    "    tracked_keypoints = [] \n",
    "    \n",
    "    keypoint_colors = {}\n",
    "    matched_kp_colors = []\n",
    "    #fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    #fps = 20 \n",
    "    #frame_size = (128, 128)\n",
    "    #out = cv2.VideoWriter('tracked_keypoints_video_2.avi', fourcc, fps, frame_size)\n",
    "    \n",
    "    for i in range(len(image_array) - 1):\n",
    "        \n",
    "        img1 = Image.fromarray(np.uint8(image_array[i]))\n",
    "        img1 = img1.convert('RGB')\n",
    "        img1 = img1.resize((image_size, image_size), Image.BICUBIC)\n",
    "        trans = transforms.ToTensor()\n",
    "        data1 = trans(img1)\n",
    "        data1 = data1.unsqueeze(0).to(device)\n",
    "\n",
    "        img2 = Image.fromarray(np.uint8(image_array[i + 1]))\n",
    "        img2 = img2.convert('RGB')\n",
    "        img2 = img2.resize((image_size, image_size), Image.BICUBIC)\n",
    "        data2 = trans(img2)\n",
    "        data2 = data2.unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model_output_1 = model(data1, x_prior=data1)\n",
    "            model_output_2 = model(data2, x_prior=data2)\n",
    "\n",
    "            mu_coords_1 = model_output_1['mu']\n",
    "            mu_coords_2 = model_output_2['mu']\n",
    "            mu_features_1 = model_output_1['mu_features']\n",
    "            mu_features_2 = model_output_2['mu_features']\n",
    "\n",
    "            '''\n",
    "            logvar_1 = model_output_1['logvar']\n",
    "            logvar_sum_1 = logvar_1.sum(-1)\n",
    "            logvar_topk_1 = torch.topk(logvar_sum_1, k=10, dim=-1, largest=False)\n",
    "            indices_1 = logvar_topk_1[1]  # [batch_size, topk]\n",
    "            batch_indices_1 = torch.arange(mu_coords_1.shape[0]).view(-1, 1).to(mu_coords_1.device)\n",
    "            topk_kp_1 = mu_coords_1[batch_indices_1, indices_1]\n",
    "\n",
    "            logvar_2 = model_output_2['logvar']\n",
    "            logvar_sum_2 = logvar_2.sum(-1)\n",
    "            logvar_topk_2 = torch.topk(logvar_sum_2, k=10, dim=-1, largest=False)\n",
    "            indices_2 = logvar_topk_2[1]  # [batch_size, topk]\n",
    "            batch_indices_2 = torch.arange(mu_coords_2.shape[0]).view(-1, 1).to(mu_coords_2.device)\n",
    "            topk_kp_2 = mu_coords_2[batch_indices_2, indices_2]\n",
    "\n",
    "            mu_1 = list(zip(topk_kp_1[0].cpu().numpy(), mu_features_1[0].cpu().numpy()))\n",
    "            mu_2 = list(zip(topk_kp_2[0].cpu().numpy(), mu_features_2[0].cpu().numpy()))\n",
    "            '''\n",
    "            mu_1 = list(zip(mu_coords_1[0].cpu().numpy(), mu_features_1[0].cpu().numpy()))\n",
    "            mu_2 = list(zip(mu_coords_2[0].cpu().numpy(), mu_features_2[0].cpu().numpy()))\n",
    "\n",
    "\n",
    "            matched_KPs = []\n",
    "            for kp_1, feature_1 in mu_1:\n",
    "                best_match = None\n",
    "                best_sim = float('inf')\n",
    "                for kp_2, feature_2 in mu_2:\n",
    "                    # Calculate similarity\n",
    "                    geometric_sim = np.linalg.norm(kp_1 - kp_2)\n",
    "                    cos_sim = np.dot(feature_1, feature_2) / (np.linalg.norm(feature_1) * np.linalg.norm(feature_2))\n",
    "                    semantic_sim = abs(1 - cos_sim)\n",
    "                    total_sim = geometric_sim + semantic_sim\n",
    "                    if total_sim < best_sim:\n",
    "                        best_sim = total_sim\n",
    "                        best_match = kp_2\n",
    "                sim_threshold = 0.2\n",
    "                if best_sim < sim_threshold:\n",
    "                    matched_KPs.append((kp_1, best_match, best_sim))\n",
    "\n",
    "            # Filter keypoints to ensure one-to-one associations\n",
    "            matched_KPs.sort(key=lambda x: x[2])\n",
    "            matched_KPs_filtered = []\n",
    "            matched_kp_1 = []\n",
    "            matched_kp_2 = []\n",
    "            for kp_1, kp_2, sim in matched_KPs:\n",
    "                if (kp_1[0], kp_1[1]) not in matched_kp_1 and (kp_2[0], kp_2[1]) not in matched_kp_2: \n",
    "                    matched_KPs_filtered.append((kp_1, kp_2, sim))\n",
    "                    matched_kp_1.append((kp_1[0], kp_1[1]))\n",
    "                    matched_kp_2.append((kp_2[0], kp_2[1]))\n",
    "            \n",
    "\n",
    "        matched_kp_tuple = []\n",
    "        for kp_1, kp_2, sim in matched_KPs_filtered:\n",
    "            kp_1 = ((kp_1 - kp_range[0]) / (kp_range[1] - kp_range[0])).clip(0, 1)\n",
    "            kp_2 = ((kp_2 - kp_range[0]) / (kp_range[1] - kp_range[0])).clip(0, 1)\n",
    "\n",
    "            prev_kp = tuple(kp_1)\n",
    "            next_kp = tuple(kp_2)\n",
    "            if prev_kp in keypoint_colors:\n",
    "                color = keypoint_colors[prev_kp]\n",
    "                keypoint_colors[next_kp] = color\n",
    "            else:\n",
    "                color = tuple(np.random.randint(0, 256, size=3))\n",
    "                keypoint_colors[next_kp] = color\n",
    "\n",
    "            kp_match_color = {'kp_1' : kp_1, \n",
    "                              'kp_2' : kp_2,\n",
    "                              'color' : color}\n",
    "            matched_kp_tuple.append(kp_match_color)\n",
    "\n",
    "\n",
    "        tracked_keypoints.append({'kps' : matched_kp_tuple, \n",
    "                                  'img' : data2[0]})\n",
    "\n",
    "    return tracked_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder backbone: gauss_pointnetpp\n",
      "prior std: 0.1\n",
      "keypoints range: (-1, 1)\n",
      "total number of kp: 64 -> prior kp: 20\n",
      "number of kp from encoder: 10\n",
      "kp_activation: tanh\n",
      "learnable feature dim: 10\n",
      "mask threshold: 0.2\n",
      "object patch size: 32\n",
      "object encoder: True, object decoder: True\n",
      "learn particles order: False\n",
      "conv shape:  torch.Size([256, 16, 16])\n",
      "conv shape:  torch.Size([32, 8, 8])\n",
      "conv shape:  torch.Size([64, 4, 4])\n",
      "YES\n",
      "loaded model from checkpoint\n",
      "52752\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "use_logsoftmax = False\n",
    "pad_mode = 'replicate'\n",
    "sigma = 0.1\n",
    "dropout = 0.0\n",
    "n_kp = 1\n",
    "kp_range = (-1, 1)\n",
    "kp_activation = \"tanh\"\n",
    "mask_threshold = 0.2\n",
    "learn_order = False\n",
    "path_to_model_ckpt = './checkpoints/calvin_dlp_gauss_pointnetpp_best.pth'\n",
    "image_size = 128\n",
    "ch = 3\n",
    "enc_channels = [32, 64, 128, 256]\n",
    "prior_channels = (16, 32, 64)\n",
    "imwidth = 160\n",
    "crop = 16\n",
    "n_kp_enc = 10 \n",
    "n_kp_prior = 20 \n",
    "use_object_enc = True\n",
    "use_object_dec = True\n",
    "learned_feature_dim = 10\n",
    "patch_size = 16\n",
    "anchor_s = 0.25\n",
    "dec_bone = \"gauss_pointnetpp\"\n",
    "exclusive_patches = False\n",
    "\n",
    "model = KeyPointVAE(cdim=ch, enc_channels=enc_channels, prior_channels=prior_channels,\n",
    "                        image_size=image_size, n_kp=n_kp, learned_feature_dim=learned_feature_dim,\n",
    "                        use_logsoftmax=use_logsoftmax, pad_mode=pad_mode, sigma=sigma,\n",
    "                        dropout=dropout, dec_bone=dec_bone, patch_size=patch_size, n_kp_enc=n_kp_enc,\n",
    "                        n_kp_prior=n_kp_prior, kp_range=kp_range, kp_activation=kp_activation,\n",
    "                        mask_threshold=mask_threshold, use_object_enc=use_object_enc,\n",
    "                        exclusive_patches=exclusive_patches, use_object_dec=use_object_dec, anchor_s=anchor_s,\n",
    "                        learn_order=learn_order).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(path_to_model_ckpt, map_location=device), strict=False)\n",
    "print(\"loaded model from checkpoint\")\n",
    "\n",
    "path = '/media/tim/E/datasets/task_D_D/validation'\n",
    "target_ann = \"open the drawer\"\n",
    "start_index = getIndex(path, target_ann)\n",
    "print(start_index)\n",
    "image_array = []       \n",
    "for i in range(0, 200):\n",
    "    data = np.load(f\"{path}/episode_{start_index + i:07d}.npz\", allow_pickle=True)\n",
    "    img = data['rgb_static']   \n",
    "    image_array.append(img)\n",
    "\n",
    "# trackedPoints = trackKeyPoints(image_array, model)\n",
    "# print(trackedPoints[0])\n",
    "\n",
    "\n",
    "tracked_points = trackKeyPoints_2(image_array, model)\n",
    "# print(tracked_points)\n",
    "\n",
    "video_img_array = create_tracked_kp_video(tracked_points)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "fps = 20 \n",
    "frame_size = (128, 128)\n",
    "out = cv2.VideoWriter('tracked_keypoints_video_7.avi', fourcc, fps, frame_size)\n",
    "for img in video_img_array:\n",
    "  #  plt.imshow(img)\n",
    "  #  plt.axis('off') \n",
    "  #  plt.show()  \n",
    "    out.write(img[:, :, ::-1])\n",
    "out.release()    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
